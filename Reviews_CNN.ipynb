{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from time import time\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"data_cnn/tcenr_data/\"#\"data_cnn/amazon_home/\"#/amazon/\"\n",
    "OUTPUT_FILE = DIR + \"cnn_best.ckpt\"\n",
    "CHECKPOINT_FILE = DIR + \"cnn_curr.ckpt\"\n",
    "WORD_EMB = 50\n",
    "REVIEW_WORDS = 500\n",
    "TRAIN_SIZE = 125376#172244#1869470#3959195#2534578#5397476\n",
    "TEST_SIZE = 13852#19232#206717#440201#281164#598485\n",
    "DICT_SIZE = 58488#57463#125410#125410#148240#148870\n",
    "#TRAIN_SIZE = 621600\n",
    "#TEST_SIZE = 69228\n",
    "#DICT_SIZE = 83586\n",
    "FILTER_SIZES = [3]\n",
    "NUM_FILTERS = 100\n",
    "LATENT_SIZE = 32\n",
    "CLASSES = 5\n",
    "LR=0.01\n",
    "EPOCHS=100\n",
    "MIN_EPOCH_TO_SAVE = 3\n",
    "MSE_DIFF_TO_SAVE = 1.002\n",
    "EARLY_STOP_INTERVAL = 10\n",
    "BATCH_SIZE = 8192\n",
    "RESTORE = True\n",
    "EXPORT_TOPIC_EMB = True\n",
    "LDA_TOPICS = 50\n",
    "LDA_DIR = \"data_cnn/lda/tcenr_data/\"#\"data_cnn/lda/amazon_home/\"#amazon/\"#\"/models_\"+ str(LDA_TOPICS) +\"/\"#\"lda_results/rest/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_review = tf.placeholder(tf.int32, [None, REVIEW_WORDS], name=\"in_review\")\n",
    "in_score = tf.placeholder(tf.int32,[None], name=\"in_score\")\n",
    "\n",
    "random_uniform_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "with tf.name_scope(\"review_embedding\"):\n",
    "    W1 = tf.get_variable(\"W_emb\",shape = [DICT_SIZE, WORD_EMB], dtype=tf.float32, initializer=random_uniform_initializer)\n",
    "    embedded_review = tf.nn.embedding_lookup(W1, in_review)\n",
    "    embedded_reviews = tf.expand_dims(embedded_review, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_normal_initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "constant_initializer_filter = tf.constant(0.1, shape=[NUM_FILTERS])\n",
    "\n",
    "pooled_outputs = []\n",
    "\n",
    "for i, filter_size in enumerate(FILTER_SIZES):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "        # Convolution Layer\n",
    "        filter_shape = [filter_size, WORD_EMB, 1, NUM_FILTERS]\n",
    "        W = tf.get_variable(\"W_\"+str(i),shape = filter_shape, dtype=tf.float32, initializer=truncated_normal_initializer)\n",
    "        b = tf.get_variable(\"b_\"+ str(i), dtype=tf.float32, initializer=constant_initializer_filter )\n",
    "        conv = tf.nn.conv2d(embedded_reviews,W,strides=[1, 1, 1, 1],padding=\"VALID\",name=\"conv_{}\".format(i))\n",
    "        # Apply nonlinearity\n",
    "        h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "        # Maxpooling over the outputs\n",
    "        pooled = tf.nn.max_pool(h,ksize=[1, REVIEW_WORDS - filter_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID',\n",
    "                                name=\"pool_{}\".format(i))\n",
    "        pooled_outputs.append(pooled)\n",
    "num_filters_total = NUM_FILTERS * len(FILTER_SIZES)\n",
    "h_pool = tf.concat(pooled_outputs,3)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier_initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat, 1.0)\n",
    "with tf.name_scope(\"get_fea\"):\n",
    "    Wh = tf.get_variable(\"Wh\",shape=[num_filters_total, LATENT_SIZE],initializer=xavier_initializer,dtype=tf.float32)\n",
    "    bh = tf.get_variable(\"bh\",initializer=tf.constant(0.1, shape=[LATENT_SIZE]), dtype=tf.float32)\n",
    "    f_fea=tf.matmul(h_drop, Wh) + bh\n",
    "    h_final = tf.nn.relu(f_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wout = tf.get_variable(\"Wout\",shape=[LATENT_SIZE,1],initializer=xavier_initializer,dtype=tf.float32)\n",
    "bout = tf.get_variable(\"bout\",initializer = tf.constant(0.1, shape=[1]),dtype=tf.float32)\n",
    "predictions_raw = tf.matmul(h_final,Wout) + bout\n",
    "predictions = tf.squeeze(predictions_raw)\n",
    "loss = tf.losses.mean_squared_error(labels=in_score,predictions=predictions)\n",
    "train_step = tf.train.AdamOptimizer(LR).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model from data_cnn/tcenr_data/cnn_best.ckpt\n",
      "INFO:tensorflow:Restoring parameters from data_cnn/tcenr_data/cnn_best.ckpt\n"
     ]
    }
   ],
   "source": [
    "def get_feed_dict(dataset,start,end,pad_size):\n",
    "    tmp_reviews = []\n",
    "    for r in dataset[\"reviews\"][start:end]:\n",
    "        to_pad = pad_size - len(r)\n",
    "        if to_pad>0:\n",
    "            r = r + [0] * to_pad\n",
    "        tmp_reviews.append(r)\n",
    "    feed_dict = {in_review: tmp_reviews, in_score: dataset[\"scores\"][start:end]}\n",
    "    return feed_dict\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if RESTORE:\n",
    "    print(\"Restoring model from \" + OUTPUT_FILE)\n",
    "    saver.restore(sess,OUTPUT_FILE)\n",
    "else:\n",
    "    print(\"Training the model from scratch\")\n",
    "    glove = pickle.load(open(DIR + \"glove_file.pkl\",\"rb\"))\n",
    "    glove_init = np.array(glove)\n",
    "    train = pickle.load(open(DIR + \"train.pkl\",\"rb\"))\n",
    "    test = pickle.load(open(DIR + \"test.pkl\",\"rb\"))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(W1.assign(glove_init))\n",
    "    best_mse=9999\n",
    "    best_mse_epoch=0\n",
    "\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        t1 = time()\n",
    "        # Shuffle the training batch order\n",
    "        training_indexes = list(range(0, TRAIN_SIZE, BATCH_SIZE))\n",
    "        total_batches = len(training_indexes)\n",
    "        np.random.shuffle(training_indexes)\n",
    "        train_ratings=np.zeros(TRAIN_SIZE)\n",
    "        train_outputs=np.zeros(TRAIN_SIZE)\n",
    "        train_losses=np.zeros(total_batches)\n",
    "        curr_iter=0\n",
    "        # Train the model for each batch size\n",
    "        for start in training_indexes:\n",
    "            end = min(start + BATCH_SIZE,TRAIN_SIZE)\n",
    "            feed_dict = get_feed_dict(train,start,end,REVIEW_WORDS) \n",
    "            # Perform a training step for current batch\n",
    "            _,curr_loss,curr_ratings, curr_output = sess.run([train_step,loss,in_score,predictions],feed_dict)\n",
    "            train_ratings[start:end] = curr_ratings\n",
    "            train_outputs[start:end] = curr_output\n",
    "            train_losses[curr_iter] = curr_loss\n",
    "            curr_iter+=1\n",
    "            #print(\"batch {} / {}, took {}\".format(curr_iter,total_batches,time()-t1))\n",
    "            \n",
    "        saver.save(sess, CHECKPOINT_FILE)\n",
    "        train_mse = mean_squared_error(y_true=train_ratings, y_pred=train_outputs)\n",
    "        \n",
    "        test_indexes = list(range(0, TEST_SIZE, BATCH_SIZE))\n",
    "        test_ratings = np.zeros(TEST_SIZE)\n",
    "        test_outputs = np.zeros(TEST_SIZE)\n",
    "        for start in test_indexes:\n",
    "            end = min(start+BATCH_SIZE, TEST_SIZE)\n",
    "            feed_dict = get_feed_dict(test,start,end,REVIEW_WORDS)        \n",
    "            curr_test_ratings, curr_test_output = sess.run([in_score, predictions],feed_dict)\n",
    "            test_ratings[start:end] = curr_test_ratings\n",
    "            test_outputs[start:end] = curr_test_output\n",
    "        test_mse = mean_squared_error(y_true=test_ratings, y_pred=test_outputs)\n",
    "        print(\"epoch {}:{} ms. avg loss {:.4f}. train MSE: {:.4f}, test MSE: {:.4f}\".format(epoch,time()-t1,\n",
    "                                                                                            np.average(train_losses), \n",
    "                                                                                            train_mse, test_mse))\n",
    "\n",
    "        if (test_mse/best_mse)<MSE_DIFF_TO_SAVE and epoch>=MIN_EPOCH_TO_SAVE:\n",
    "            save_path = saver.save(sess, OUTPUT_FILE)\n",
    "            print(\"MSE improved from {:.4f} to {:.4f}. Model saved to {}\".format(best_mse,test_mse,save_path))\n",
    "            best_mse = test_mse\n",
    "            best_mse_epoch = epoch\n",
    "\n",
    "        if (epoch - best_mse_epoch)>EARLY_STOP_INTERVAL:\n",
    "            print(\"Early stop due to no imporvement since epoch {}\".format(best_mse_epoch))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13852"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open(DIR + \"test.pkl\",\"rb\"))\n",
    "test_indexes = list(range(0, TEST_SIZE, BATCH_SIZE))\n",
    "test_ratings = np.zeros(TEST_SIZE)\n",
    "test_outputs = np.zeros(TEST_SIZE)\n",
    "for start in test_indexes:\n",
    "    end = min(start+BATCH_SIZE, TEST_SIZE)\n",
    "    feed_dict = get_feed_dict(test,start,end,REVIEW_WORDS)        \n",
    "    curr_test_ratings, curr_test_output = sess.run([in_score, predictions],feed_dict)\n",
    "    test_ratings[start:end] = curr_test_ratings\n",
    "    test_outputs[start:end] = curr_test_output\n",
    "test_mse = mean_squared_error(y_true=test_ratings, y_pred=test_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5485044573951946\n"
     ]
    }
   ],
   "source": [
    "print(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1 / 229\n",
      "batch 2 / 229\n",
      "batch 3 / 229\n",
      "batch 4 / 229\n",
      "batch 5 / 229\n",
      "batch 6 / 229\n",
      "batch 7 / 229\n",
      "batch 8 / 229\n",
      "batch 9 / 229\n",
      "batch 10 / 229\n",
      "batch 11 / 229\n",
      "batch 12 / 229\n",
      "batch 13 / 229\n",
      "batch 14 / 229\n",
      "batch 15 / 229\n",
      "batch 16 / 229\n",
      "batch 17 / 229\n",
      "batch 18 / 229\n",
      "batch 19 / 229\n",
      "batch 20 / 229\n",
      "batch 21 / 229\n",
      "batch 22 / 229\n",
      "batch 23 / 229\n",
      "batch 24 / 229\n",
      "batch 25 / 229\n",
      "batch 26 / 229\n",
      "batch 27 / 229\n",
      "batch 28 / 229\n",
      "batch 29 / 229\n",
      "batch 30 / 229\n",
      "batch 31 / 229\n",
      "batch 32 / 229\n",
      "batch 33 / 229\n",
      "batch 34 / 229\n",
      "batch 35 / 229\n",
      "batch 36 / 229\n",
      "batch 37 / 229\n",
      "batch 38 / 229\n",
      "batch 39 / 229\n",
      "batch 40 / 229\n",
      "batch 41 / 229\n",
      "batch 42 / 229\n",
      "batch 43 / 229\n",
      "batch 44 / 229\n",
      "batch 45 / 229\n",
      "batch 46 / 229\n",
      "batch 47 / 229\n",
      "batch 48 / 229\n",
      "batch 49 / 229\n",
      "batch 50 / 229\n",
      "batch 51 / 229\n",
      "batch 52 / 229\n",
      "batch 53 / 229\n",
      "batch 54 / 229\n",
      "batch 55 / 229\n",
      "batch 56 / 229\n",
      "batch 57 / 229\n",
      "batch 58 / 229\n",
      "batch 59 / 229\n",
      "batch 60 / 229\n",
      "batch 61 / 229\n",
      "batch 62 / 229\n",
      "batch 63 / 229\n",
      "batch 64 / 229\n",
      "batch 65 / 229\n",
      "batch 66 / 229\n",
      "batch 67 / 229\n",
      "batch 68 / 229\n",
      "batch 69 / 229\n",
      "batch 70 / 229\n",
      "batch 71 / 229\n",
      "batch 72 / 229\n",
      "batch 73 / 229\n",
      "batch 74 / 229\n",
      "batch 75 / 229\n",
      "batch 76 / 229\n",
      "batch 77 / 229\n",
      "batch 78 / 229\n",
      "batch 79 / 229\n",
      "batch 80 / 229\n",
      "batch 81 / 229\n",
      "batch 82 / 229\n",
      "batch 83 / 229\n",
      "batch 84 / 229\n",
      "batch 85 / 229\n",
      "batch 86 / 229\n",
      "batch 87 / 229\n",
      "batch 88 / 229\n",
      "batch 89 / 229\n",
      "batch 90 / 229\n",
      "batch 91 / 229\n",
      "batch 92 / 229\n",
      "batch 93 / 229\n",
      "batch 94 / 229\n",
      "batch 95 / 229\n",
      "batch 96 / 229\n",
      "batch 97 / 229\n",
      "batch 98 / 229\n",
      "batch 99 / 229\n",
      "batch 100 / 229\n",
      "batch 101 / 229\n",
      "batch 102 / 229\n",
      "batch 103 / 229\n",
      "batch 104 / 229\n",
      "batch 105 / 229\n",
      "batch 106 / 229\n",
      "batch 107 / 229\n",
      "batch 108 / 229\n",
      "batch 109 / 229\n",
      "batch 110 / 229\n",
      "batch 111 / 229\n",
      "batch 112 / 229\n",
      "batch 113 / 229\n",
      "batch 114 / 229\n",
      "batch 115 / 229\n",
      "batch 116 / 229\n",
      "batch 117 / 229\n",
      "batch 118 / 229\n",
      "batch 119 / 229\n",
      "batch 120 / 229\n",
      "batch 121 / 229\n",
      "batch 122 / 229\n",
      "batch 123 / 229\n",
      "batch 124 / 229\n",
      "batch 125 / 229\n",
      "batch 126 / 229\n",
      "batch 127 / 229\n",
      "batch 128 / 229\n",
      "batch 129 / 229\n",
      "batch 130 / 229\n",
      "batch 131 / 229\n",
      "batch 132 / 229\n",
      "batch 133 / 229\n",
      "batch 134 / 229\n",
      "batch 135 / 229\n",
      "batch 136 / 229\n",
      "batch 137 / 229\n",
      "batch 138 / 229\n",
      "batch 139 / 229\n",
      "batch 140 / 229\n",
      "batch 141 / 229\n",
      "batch 142 / 229\n",
      "batch 143 / 229\n",
      "batch 144 / 229\n",
      "batch 145 / 229\n",
      "batch 146 / 229\n",
      "batch 147 / 229\n",
      "batch 148 / 229\n",
      "batch 149 / 229\n",
      "batch 150 / 229\n",
      "batch 151 / 229\n",
      "batch 152 / 229\n",
      "batch 153 / 229\n",
      "batch 154 / 229\n",
      "batch 155 / 229\n",
      "batch 156 / 229\n",
      "batch 157 / 229\n",
      "batch 158 / 229\n",
      "batch 159 / 229\n",
      "batch 160 / 229\n",
      "batch 161 / 229\n",
      "batch 162 / 229\n",
      "batch 163 / 229\n",
      "batch 164 / 229\n",
      "batch 165 / 229\n",
      "batch 166 / 229\n",
      "batch 167 / 229\n",
      "batch 168 / 229\n",
      "batch 169 / 229\n",
      "batch 170 / 229\n",
      "batch 171 / 229\n",
      "batch 172 / 229\n",
      "batch 173 / 229\n",
      "batch 174 / 229\n",
      "batch 175 / 229\n",
      "batch 176 / 229\n",
      "batch 177 / 229\n",
      "batch 178 / 229\n",
      "batch 179 / 229\n",
      "batch 180 / 229\n",
      "batch 181 / 229\n",
      "batch 182 / 229\n",
      "batch 183 / 229\n",
      "batch 184 / 229\n",
      "batch 185 / 229\n",
      "batch 186 / 229\n",
      "batch 187 / 229\n",
      "batch 188 / 229\n",
      "batch 189 / 229\n",
      "batch 190 / 229\n",
      "batch 191 / 229\n",
      "batch 192 / 229\n",
      "batch 193 / 229\n",
      "batch 194 / 229\n",
      "batch 195 / 229\n",
      "batch 196 / 229\n",
      "batch 197 / 229\n",
      "batch 198 / 229\n",
      "batch 199 / 229\n",
      "batch 200 / 229\n",
      "batch 201 / 229\n",
      "batch 202 / 229\n",
      "batch 203 / 229\n",
      "batch 204 / 229\n",
      "batch 205 / 229\n",
      "batch 206 / 229\n",
      "batch 207 / 229\n",
      "batch 208 / 229\n",
      "batch 209 / 229\n",
      "batch 210 / 229\n",
      "batch 211 / 229\n",
      "batch 212 / 229\n",
      "batch 213 / 229\n",
      "batch 214 / 229\n",
      "batch 215 / 229\n",
      "batch 216 / 229\n",
      "batch 217 / 229\n",
      "batch 218 / 229\n",
      "batch 219 / 229\n",
      "batch 220 / 229\n",
      "batch 221 / 229\n",
      "batch 222 / 229\n",
      "batch 223 / 229\n",
      "batch 224 / 229\n",
      "batch 225 / 229\n",
      "batch 226 / 229\n",
      "batch 227 / 229\n",
      "batch 228 / 229\n",
      "batch 229 / 229\n",
      "0.322964879649003\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "test = None\n",
    "gc.collect()\n",
    "train = pickle.load(open(DIR + \"train.pkl\",\"rb\"))\n",
    "training_indexes = list(range(0, TRAIN_SIZE, BATCH_SIZE))\n",
    "total_batches = len(training_indexes)\n",
    "np.random.shuffle(training_indexes)\n",
    "train_ratings=np.zeros(TRAIN_SIZE)\n",
    "train_outputs=np.zeros(TRAIN_SIZE)\n",
    "curr_iter=0\n",
    "# Train the model for each batch size\n",
    "for start in training_indexes:\n",
    "    end = min(start + BATCH_SIZE,TRAIN_SIZE)\n",
    "    feed_dict = get_feed_dict(train,start,end,REVIEW_WORDS) \n",
    "    # Perform a training step for current batch\n",
    "    curr_ratings, curr_output = sess.run([in_score,predictions],feed_dict)\n",
    "    train_ratings[start:end] = curr_ratings\n",
    "    train_outputs[start:end] = curr_output\n",
    "    curr_iter+=1\n",
    "    print(\"batch {} / {}\".format(curr_iter,total_batches))\n",
    "train_mse = mean_squared_error(y_true=train_ratings, y_pred=train_outputs)\n",
    "print(train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_cnn/lda/tcenr_data/corpus.lda-c\n"
     ]
    }
   ],
   "source": [
    "if EXPORT_TOPIC_EMB:\n",
    "    topics_lst = [50]\n",
    "    for LDA_TOPICS in topics_lst:\n",
    "        dictionary_path = LDA_DIR + \"dictionary_\" + str(LDA_TOPICS) + \".dict\"\n",
    "        corpus_path = LDA_DIR + \"corpus\"+ \".lda-c\"\n",
    "        dictionary = corpora.Dictionary.load(dictionary_path)\n",
    "        print(corpus_path)\n",
    "        corpus = corpora.BleiCorpus(corpus_path)\n",
    "        lda_model_path = LDA_DIR + \"lda_model_\" + str(LDA_TOPICS) + \"_topics.lda\"\n",
    "\n",
    "        word_dict = pickle.load(open(DIR + \"word_dictionary.pkl\",\"rb\"))\n",
    "        word_dict_rev = pickle.load(open(DIR + \"word_dictionary_reverse.pkl\",\"rb\"))\n",
    "\n",
    "        dictionary = corpora.Dictionary.load(dictionary_path)\n",
    "        corpus = corpora.BleiCorpus(corpus_path)\n",
    "        lda = LdaModel.load(lda_model_path)\n",
    "\n",
    "        W1_arr = sess.run([W1])\n",
    "        word_embeddings = W1_arr[0]\n",
    "\n",
    "        #get raw topic > word estimates\n",
    "        topics_terms = lda.state.get_lambda() \n",
    "\n",
    "        #convert estimates to probability (sum equals to 1 per topic)\n",
    "        topics_terms_proba = np.apply_along_axis(lambda x: x/x.sum(),1,topics_terms)\n",
    "\n",
    "        # find the right word based on column index\n",
    "        words = [lda.id2word[i] for i in range(topics_terms_proba.shape[1])]\n",
    "\n",
    "        words_per_topic = len(words)\n",
    "        #topic_embeddings_raw = np.empty((LDA_TOPICS,words_per_topic,WORD_EMB))\n",
    "        #topic_embeddings_raw[:] = np.nan\n",
    "        topic_embeddings_raw = np.zeros((LDA_TOPICS,words_per_topic,WORD_EMB))\n",
    "\n",
    "        for t in range(len(topics_terms_proba)):\n",
    "            topic_dist = topics_terms_proba[t]\n",
    "            for w in range(len(topic_dist)):\n",
    "                word = words[w]\n",
    "                if word in word_dict:\n",
    "                    emb_idx = word_dict[word]\n",
    "                    word_vec = word_embeddings[emb_idx]\n",
    "                    topic_weight = topic_dist[w]\n",
    "                    topic_embeddings_raw[t][w] = topic_weight * word_vec\n",
    "\n",
    "        #topic_embeddings = np.nanmean(topic_embeddings_raw, axis=1)\n",
    "        topic_embeddings = np.sum(topic_embeddings_raw, axis=1)\n",
    "\n",
    "        with open(DIR + \"topic_embeddings_\" +str(LDA_TOPICS) + \".pkl\",'wb') as file:\n",
    "            pickle.dump(topic_embeddings,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
