{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing record 100000\n",
      "Loaded 44898 users, 3954 locations and 99999 reviews\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-400e082b2d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"overall\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reviewText\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on Jul 27, 2018\n",
    "\n",
    "@author: omert\n",
    "'''\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "from operator import itemgetter\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "from datetime import datetime\n",
    "\n",
    "TEXT = True\n",
    "#DIR = \"/home/omertl123/yelp/dataset/\"\n",
    "DIR = \"../data/\"\n",
    "AMAZON_TYPE=\"Home_and_Kitchen\"\n",
    "OUTPUT_DIR = \"amazon_dataset/\" + AMAZON_TYPE + \"/\"\n",
    "FILE_NAME = DIR+\"reviews_\" + AMAZON_TYPE + \"_5.json\"#\"Electronics_5.json\"\n",
    "reviews_count = 0\n",
    "users = {}\n",
    "pois = {}\n",
    "reviews_file = open(FILE_NAME,encoding=\"utf8\")\n",
    "users_count = 0\n",
    "data_loaded = 0\n",
    "poi_count = 0\n",
    "temp_data = []\n",
    "for line in reviews_file:\n",
    "    data_loaded+=1\n",
    "    if (data_loaded % 100000 == 0):\n",
    "        print(\"Processing record {}\".format(data_loaded))\n",
    "        print(\"Loaded {} users, {} locations and {} reviews\".format(users_count,poi_count,reviews_count))\n",
    "    record = json.loads(line)\n",
    "    user_id = record[\"reviewerID\"]\n",
    "    poi_id = record[\"asin\"]\n",
    "    ts = record[\"unixReviewTime\"]\n",
    "    score = float(record[\"overall\"])\n",
    "    date = str(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d'))\n",
    "    text = record[\"reviewText\"].replace(\"\\t\",\"\").replace(\"\\n\",\"\")\n",
    "    if score>=0:\n",
    "        if user_id not in users:\n",
    "            users[user_id]=0\n",
    "            users_count+=1\n",
    "        if poi_id not in pois:\n",
    "            pois[poi_id]=0\n",
    "            poi_count+=1\n",
    "        if user_id in users and poi_id in pois:\n",
    "            if TEXT:\n",
    "                temp_data.append((user_id,poi_id,date,text))\n",
    "            else:\n",
    "                temp_data.append((user_id,poi_id,date,\"\"))\n",
    "            users[user_id]+=1\n",
    "            pois[poi_id]+=1\n",
    "            reviews_count+=1\n",
    "\n",
    "reviews_file.close()\n",
    "\n",
    "print(\"Loaded {} reviews\".format(reviews_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "POI_LIMIT = 0\n",
    "MIN_REVIEWS_POI = 10\n",
    "USER_LIMIT = 0\n",
    "MIN_REVIEWS_USER = 15\n",
    "TOP_TOPICS = 25\n",
    "MIN_USERS_FOR_SET = 15\n",
    "NEG_RECORDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 879878 output records with 21756 users and 30761 locations\n",
      "sparsity:0.00459643316786174%\n"
     ]
    }
   ],
   "source": [
    "if POI_LIMIT>0:\n",
    "    pois_sorted = sorted(pois.items(), key=itemgetter(1),reverse=True)\n",
    "    counter=0\n",
    "    for key,value in pois_sorted:\n",
    "        counter+=1\n",
    "        if (counter>POI_LIMIT):\n",
    "            pois[key] = 0\n",
    "\n",
    "if USER_LIMIT>0:\n",
    "    users_sorted = sorted(users.items(), key=itemgetter(1),reverse=True)\n",
    "    counter=0\n",
    "    for key,value in users_sorted:\n",
    "        counter+=1\n",
    "        if (counter>USER_LIMIT):\n",
    "            users[key] = 0\n",
    "        \n",
    "filtered_data = {}\n",
    "filtered_records = 0\n",
    "user_indexes = {}\n",
    "user_index_rev = {}\n",
    "poi_indexes = {}\n",
    "poi_index_rev = {}\n",
    "user_counter = 0\n",
    "poi_counter = 0\n",
    "user_distinct = {}\n",
    "for record in temp_data:\n",
    "    user_id = record[0]\n",
    "    poi_id = record[1]\n",
    "    date = record[2]\n",
    "    text = record[3]\n",
    "    if (users[user_id] >= MIN_REVIEWS_USER and pois[poi_id]>=MIN_REVIEWS_POI):\n",
    "        if user_id not in user_indexes:\n",
    "            user_indexes[user_id] = user_counter\n",
    "            user_index_rev[user_counter] = user_id\n",
    "            user_counter += 1\n",
    "        if poi_id not in poi_indexes:\n",
    "            poi_indexes[poi_id] = poi_counter\n",
    "            poi_index_rev[poi_counter] = poi_id\n",
    "            poi_counter += 1\n",
    "        user_idx = user_indexes[user_id]\n",
    "        poi_idx = poi_indexes[poi_id]\n",
    "        if user_idx not in filtered_data:\n",
    "            filtered_data[user_idx] = []\n",
    "            user_distinct[user_idx] = set()\n",
    "        if poi_idx not in user_distinct[user_idx]:\n",
    "            value = (poi_idx,date,text)\n",
    "            filtered_data[user_idx].append(value)\n",
    "            user_distinct[user_idx].add(poi_idx)\n",
    "        filtered_records+=1\n",
    "\n",
    "print(\"loaded {} output records with {} users and {} locations\".format(filtered_records,user_counter,poi_counter))\n",
    "print(\"sparsity:{}%\".format((poi_counter/(user_counter*poi_counter))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_record_to_file(user,data,idx,K,file,remove_text,last_k_str=\"\"):\n",
    "    if last_k_str==\"\":\n",
    "        last_k = []\n",
    "        for i in range(idx-K,idx):\n",
    "            last_k.append(data[i][0])\n",
    "        last_k_str=','.join(str(j) for j in last_k)\n",
    "    if remove_text:\n",
    "        file.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(user,data[idx][0],1,data[idx][1],last_k_str,\"\"))\n",
    "    else:\n",
    "        file.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(user,data[idx][0],1,data[idx][1],last_k_str,data[idx][2].encode(\"utf-8\")))\n",
    "    return last_k_str\n",
    "\n",
    "def write_negative_samples(user,data,file,samples,poi_count,last_k):\n",
    "    counter = 0\n",
    "    for j in range(samples):\n",
    "        randomize = True\n",
    "        while randomize:\n",
    "            r = random.randint(1,poi_count)\n",
    "            for k in range(len(data)):\n",
    "                if data[k][0] == r:\n",
    "                    break\n",
    "            if k==len(data)-1 and data[k][0]!=r:\n",
    "                randomize = False\n",
    "        file.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(user,r,0,\"2018-08-01\",last_k,\"\".encode(\"utf-8\")))\n",
    "        counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "REMOVE_TEXT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2469196 training, 78264 validation and 78264 test records\n",
      "19566 users, 30761 locations\n",
      "Wrote to 879\n"
     ]
    }
   ],
   "source": [
    "for K in [4,6,8,10,12]:\n",
    "    OUTPUT_PREFIX = OUTPUT_DIR+str(filtered_records//1000)+\"_amazon_\"\n",
    "    FILES_PREFIX = OUTPUT_PREFIX + str(K) + \"_\"\n",
    "    train_file = open(FILES_PREFIX + \"train.txt\",\"w+\")\n",
    "    test_file = open(FILES_PREFIX + \"test.txt\",\"w+\")\n",
    "    valid_file = open(FILES_PREFIX + \"valid.txt\",\"w+\")\n",
    "    training_records = 0\n",
    "    test_records = 0\n",
    "    users_wrote = 0\n",
    "    valid_records = 0\n",
    "    max_poi_index = len(poi_indexes) - 1\n",
    "    user_index2 = {}\n",
    "    user_index_rev2 = {}\n",
    "    for user_raw in filtered_data:\n",
    "        data = filtered_data[user_raw]\n",
    "        data.sort(key=itemgetter(1))\n",
    "        if len(data)>=MIN_USERS_FOR_SET:\n",
    "            user = users_wrote\n",
    "            user_index2[user_raw] = user\n",
    "            user_index_rev2[users_wrote] = user_index_rev[user_raw]\n",
    "            users_wrote+=1\n",
    "            for idx in range(K,len(data)-2):\n",
    "                last_k = write_record_to_file(user,data,idx,K,train_file,REMOVE_TEXT)\n",
    "                training_records += 1\n",
    "                #negative sampling\n",
    "                training_records += write_negative_samples(user,data,train_file,NEG_RECORDS,max_poi_index,last_k)\n",
    "            last_k = write_record_to_file(user,data,len(data)-2,K,valid_file,REMOVE_TEXT)\n",
    "            valid_records+=1\n",
    "            valid_records+=write_negative_samples(user,data,valid_file,NEG_RECORDS,max_poi_index,last_k)\n",
    "            last_k = write_record_to_file(user,data,len(data)-1,K,test_file,REMOVE_TEXT,last_k)\n",
    "            test_records+=1\n",
    "            test_records+=write_negative_samples(user,data,test_file,NEG_RECORDS,max_poi_index,last_k)\n",
    "    train_file.close()\n",
    "    valid_file.close()\n",
    "    test_file.close()\n",
    "\n",
    "    print(\"Wrote {} training, {} validation and {} test records\".format(training_records,valid_records,test_records))\n",
    "    print(\"{} users, {} locations\".format(users_wrote,len(poi_indexes)))\n",
    "    print(\"Wrote to {}\".format(str(filtered_records//1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "poi_to_name = {}\n",
    "with open(DIR+\"meta_Home_and_Kitchen.json\",'r',encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        record = ast.literal_eval(line)#json.loads(line)\n",
    "        poi_id = record['asin']\n",
    "        if('title' in record):\n",
    "            poi_name = record['title']\n",
    "        else:\n",
    "            poi_name = \"Unknown\"\n",
    "        if poi_id in poi_indexes:\n",
    "            poi_to_name[poi_id] = poi_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_PREFIX + \"poi_index_rev.pkl\",\"wb+\") as f:\n",
    "    pickle.dump(poi_index_rev, f)\n",
    "with open(OUTPUT_PREFIX + \"user_index_rev.pkl\",\"wb+\") as f:\n",
    "    pickle.dump(user_index_rev2, f)\n",
    "with open(OUTPUT_PREFIX + \"poi_names.pkl\",\"wb+\") as f:\n",
    "    pickle.dump(poi_to_name, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "topics_lst = [10,25,30,50,70,90]\n",
    "for LDA_TOPICS in topics_lst:\n",
    "    LDA_PATH = \"data_cnn/lda/amazon_home/\" # + \"models_\" + str(LDA_TOPICS) + \"/\"\n",
    "\n",
    "    dictionary_path = LDA_PATH + \"dictionary_\" + str(LDA_TOPICS) + \".dict\"\n",
    "    corpus_path = LDA_PATH + \"corpus\" + str(LDA_TOPICS) + \".lda-c\"\n",
    "    dictionary = corpora.Dictionary.load(dictionary_path)\n",
    "    corpus = corpora.BleiCorpus(corpus_path)\n",
    "    lda_model_path = LDA_PATH + \"lda_model_\" + str(LDA_TOPICS) + \"_topics.lda\"\n",
    "    lda = LdaModel.load(lda_model_path)\n",
    "    poi_topics = {}\n",
    "    for user_data in filtered_data.values():\n",
    "        for record in user_data:\n",
    "            poi_id = record[0]\n",
    "            if poi_id not in poi_topics:\n",
    "                poi_topics[poi_id] = np.zeros(LDA_TOPICS)\n",
    "            review = record[2]\n",
    "            lda_topics = lda[dictionary.doc2bow(review.split(\" \"))]\n",
    "            for i in range(LDA_TOPICS):\n",
    "                poi_topics[poi_id][i] += lda_topics[i][1]\n",
    "    with open(OUTPUT_PREFIX + \"topics_\"+str(LDA_TOPICS)+\".pkl\",\"wb+\") as f:\n",
    "        pickle.dump(poi_topics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
