{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num features = 10\n",
      "Train size = 542825\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,mean_squared_error\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "def load_file(file):\n",
    "    users = []\n",
    "    pois = []\n",
    "    scores = []\n",
    "    historic_pois = []\n",
    "    reviews = []\n",
    "    for line in file:\n",
    "        record = line.split(\"\\t\")\n",
    "        user_id = int(record[0])\n",
    "        poi_id = int(record[1])\n",
    "        score = float(record[2])\n",
    "        last_k = [int(s) for s in record[4].split(',')]\n",
    "        review = record[5]\n",
    "        users.append(user_id)\n",
    "        pois.append(poi_id)\n",
    "        scores.append(score)\n",
    "        historic_pois.append(last_k)\n",
    "        reviews.append(review)\n",
    "    return users,pois,scores,historic_pois,reviews\n",
    "\n",
    "# Dataset stats\n",
    "FIRST_K = 10\n",
    "print(\"num features = {}\".format(FIRST_K))\n",
    "PREFIX = \"tcenr_data/141/\"#\"414_amazon_\"#\"414_t25_rest_b_\"#\"1655_t10_rest_\"\n",
    "INPUT_PREFIX = PREFIX + str(FIRST_K) + \"_\"\n",
    "DIR = \"yelp/\" + INPUT_PREFIX#\"amazon_dataset/\" + INPUT_PREFIX #\"yelp_dataset/\" + PREFIX\n",
    "WEIGHTS_DIR = \"result/\" + PREFIX\n",
    "#NUM_USERS = 55160#3471#38039#7987#6869  \n",
    "#NUM_ITEMS = 67144#3996#50513#6153  \n",
    "#TRAIN_SIZE = 6239812#694836#4157256#1089484 \n",
    "#TEST_SIZE = 220640#13884#152156#27476 \n",
    "NUM_USERS = 2320#14346 \n",
    "NUM_ITEMS = 2583#34389\n",
    "#TRAIN_SIZE = 1115292#885756\n",
    "TEST_SIZE = 11600#57384 \n",
    "\n",
    "sys.argv=['pretraining2.py',32,32]\n",
    "    \n",
    "# Load training data\n",
    "with open(DIR + \"train.txt\") as train_file:\n",
    "    train_users, train_pois, train_scores, train_k_pois, train_reviews  = load_file(train_file)\n",
    "    \n",
    "TRAIN_SIZE = len(train_users)\n",
    "print(\"Train size = {}\".format(TRAIN_SIZE))\n",
    "# Set training dataset\n",
    "train_set = {'users': train_users, 'pois': train_pois, 'scores': train_scores, 'k_pois': train_k_pois, 'reviews': train_reviews}\n",
    "\n",
    "# Load test data\n",
    "with open(DIR + \"valid.txt\") as test_file:\n",
    "    test_users, test_pois, test_scores, test_k_pois, test_reviews = load_file(test_file)\n",
    "# Set test dataset\n",
    "test_set = {'users': test_users, 'pois': test_pois, 'scores': test_scores, 'k_pois': test_k_pois, 'reviews': test_reviews}\n",
    "\n",
    "# Check training data\n",
    "assert len(train_users)==TRAIN_SIZE, \"mismatch in training set size {}!={}\".format(len(train_users),TRAIN_SIZE)\n",
    "assert len(train_k_pois[0])==FIRST_K==len(test_k_pois[0]), \"mismatch in number of positive items {}!={}!={}\".format(len(train_k_pois[0]),FIRST_K,len(test_k_pois[0]))\n",
    "assert max(train_users)==(NUM_USERS-1)==max(test_users), \"mimatch in number of users {}!={}!={}\".format(max(train_users),(NUM_USERS-1),max(test_users))\n",
    "assert max(train_pois)==(NUM_ITEMS-1)==max(test_pois), \"mismatch in number of item {}!={}!={}\".format(max(train_pois),(NUM_ITEMS-1),max(test_pois))\n",
    "assert len(test_users)==TEST_SIZE, \"mismatch in test set size {}!={}\".format(len(test_users),TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item and user embedding sizes: 32, 32\n"
     ]
    }
   ],
   "source": [
    "# Location embedding sizes\n",
    "if len(sys.argv)>1:\n",
    "    ITEM_EMBEDDING_SIZE = sys.argv[1]\n",
    "else:\n",
    "    ITEM_EMBEDDING_SIZE = 32#128\n",
    "\n",
    "# User embedding sizes\n",
    "if len(sys.argv)>2:\n",
    "    USER_EMBEDDING_SIZE = sys.argv[2]\n",
    "else:\n",
    "    USER_EMBEDDING_SIZE = 32\n",
    "\n",
    "print(\"item and user embedding sizes: {}, {}\".format(ITEM_EMBEDDING_SIZE,USER_EMBEDDING_SIZE))\n",
    "HIDDEN_LAYERS = [64,32]\n",
    "\n",
    "# Training parameters\n",
    "LR = 0.05\n",
    "EPOCHS = 150\n",
    "EARLY_STOP_INTERVAL = 20\n",
    "MIN_EPOCH_TO_SAVE = 10\n",
    "ROC_DIFF_TO_SAVE = 1.002\n",
    "BATCH_SIZE = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "in_item = tf.placeholder(tf.int32,[None],'in_item')\n",
    "in_user = tf.placeholder(tf.int32,[None],'in_user')\n",
    "in_ratings = tf.placeholder(tf.float32,[None],'in_ratings')\n",
    "\n",
    "init_value = 0.1\n",
    "\n",
    "emb_user_layer = tf.Variable(tf.truncated_normal([NUM_USERS, USER_EMBEDDING_SIZE], \n",
    "                                                 stddev=init_value/math.sqrt(float(USER_EMBEDDING_SIZE)), mean=0),\n",
    "                       name = 'user_embedding', dtype=tf.float32)\n",
    "emb_item_layer = tf.Variable(tf.truncated_normal([NUM_ITEMS, ITEM_EMBEDDING_SIZE], \n",
    "                                           stddev=init_value/math.sqrt(float(ITEM_EMBEDDING_SIZE)), mean=0), \n",
    "                       name = 'item_embedding', dtype=tf.float32)\n",
    "emb_user = tf.nn.embedding_lookup(emb_user_layer, in_user, name = 'target_user_emb') \n",
    "emb_item = tf.nn.embedding_lookup(emb_item_layer, in_item, name = 'candidate_item_emb')   \n",
    "        \n",
    "hidden_layers = [tf.concat([emb_user,emb_item],1)]\n",
    "\n",
    "model_params = [emb_user_layer,emb_item_layer]  \n",
    "\n",
    "for i in range(1,len(HIDDEN_LAYERS)):\n",
    "    w_hidden_layer = tf.Variable(tf.truncated_normal([HIDDEN_LAYERS[i-1],HIDDEN_LAYERS[i]], stddev = init_value, mean = 0), \n",
    "                                 name = 'w_hidden_'+ str(i), dtype=tf.float32) \n",
    "    b_hidden_layer = tf.Variable(tf.truncated_normal([HIDDEN_LAYERS[i]], stddev = init_value*0.1, mean = 0), \n",
    "                                 name = 'b_hidden_'+ str(i), dtype=tf.float32)\n",
    "    cur_layer = tf.nn.xw_plus_b(hidden_layers[i-1], w_hidden_layer, b_hidden_layer)\n",
    "    cur_layer = tf.nn.relu(cur_layer)\n",
    "    hidden_layers.append(cur_layer)\n",
    "    model_params.append(w_hidden_layer)\n",
    "    model_params.append(b_hidden_layer)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_output = tf.Variable(tf.truncated_normal([HIDDEN_LAYERS[-1], 1], stddev=init_value, mean=0), name='w_output', dtype=tf.float32)\n",
    "b_output =  tf.Variable(tf.truncated_normal([1], stddev=init_value*0.01, mean=0), name='b_output', dtype=tf.float32)\n",
    "model_params.append(w_output)\n",
    "model_params.append(b_output)\n",
    "raw_predictions = tf.nn.xw_plus_b(cur_layer, w_output, b_output, name='output')\n",
    "\n",
    "predictions = tf.reshape(tf.sigmoid(raw_predictions), [-1]) \n",
    "raw_error = tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(raw_predictions, [-1]), \n",
    "                                                    labels=tf.reshape(in_ratings, [-1]))\n",
    "error = tf.reduce_mean(raw_error,name='cross_entropy_loss')\n",
    "loss = error\n",
    "train_step = tf.train.AdamOptimizer(LR).minimize(loss, var_list=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(dataset,start,end):\n",
    "    feed_dict = {in_item : dataset['pois'][start:end], in_user : dataset['users'][start:end],\n",
    "                 in_ratings : dataset['scores'][start:end]}\n",
    "    return feed_dict\n",
    "\n",
    "def evaluate_model(dataset,set_size,batch_size):\n",
    "    all_ratings = []\n",
    "    all_predictions = []\n",
    "    for i in range(0,set_size,batch_size):\n",
    "        curr_ratings,curr_predictions = sess.run([in_ratings, predictions],get_feed_dict(dataset,i,i+batch_size))\n",
    "        all_ratings = all_ratings + curr_ratings\n",
    "        all_predictions = all_predictions + curr_predictions\n",
    "    return all_ratings,all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy before training :0.4412 mse:0.2501 roc:0.4973\n",
      "epoch 0 took 0.41890573501586914 ms. avg loss 0.4905\n",
      "Train accuracy:0.7954 mse:0.1572 roc:0.6214\n",
      "Test accuracy:0.8000 mse:0.1651 roc:0.5475\n",
      "epoch 1 took 0.37436723709106445 ms. avg loss 0.4760\n",
      "Train accuracy:0.7975 mse:0.1520 roc:0.6690\n",
      "Test accuracy:0.7984 mse:0.1549 roc:0.6477\n",
      "epoch 2 took 0.32744693756103516 ms. avg loss 0.4596\n",
      "Train accuracy:0.7990 mse:0.1474 roc:0.7081\n",
      "Test accuracy:0.7961 mse:0.1469 roc:0.7337\n",
      "epoch 3 took 0.3484196662902832 ms. avg loss 0.4522\n",
      "Train accuracy:0.7996 mse:0.1442 roc:0.7415\n",
      "Test accuracy:0.8033 mse:0.1387 roc:0.7816\n",
      "epoch 4 took 0.326401948928833 ms. avg loss 0.4104\n",
      "Train accuracy:0.8067 mse:0.1330 roc:0.7959\n",
      "Test accuracy:0.8059 mse:0.1344 roc:0.8002\n",
      "epoch 5 took 0.2804605960845947 ms. avg loss 0.3982\n",
      "Train accuracy:0.8104 mse:0.1295 roc:0.8117\n",
      "Test accuracy:0.8244 mse:0.1212 roc:0.8478\n",
      "epoch 6 took 0.2973921298980713 ms. avg loss 0.3675\n",
      "Train accuracy:0.8158 mse:0.1213 roc:0.8429\n",
      "Test accuracy:0.8273 mse:0.1208 roc:0.8561\n",
      "epoch 7 took 0.28454041481018066 ms. avg loss 0.3544\n",
      "Train accuracy:0.8197 mse:0.1180 roc:0.8540\n",
      "Test accuracy:0.8301 mse:0.1158 roc:0.8593\n",
      "epoch 8 took 0.34242916107177734 ms. avg loss 0.3495\n",
      "Train accuracy:0.8215 mse:0.1162 roc:0.8588\n",
      "Test accuracy:0.8344 mse:0.1110 roc:0.8697\n",
      "epoch 9 took 0.35770511627197266 ms. avg loss 0.3430\n",
      "Train accuracy:0.8246 mse:0.1144 roc:0.8631\n",
      "Test accuracy:0.8407 mse:0.1088 roc:0.8763\n",
      "epoch 10 took 0.3740193843841553 ms. avg loss 0.3410\n",
      "Train accuracy:0.8277 mse:0.1131 roc:0.8661\n",
      "Test accuracy:0.8345 mse:0.1112 roc:0.8691\n",
      "epoch 11 took 0.30682849884033203 ms. avg loss 0.3454\n",
      "Train accuracy:0.8257 mse:0.1144 roc:0.8632\n",
      "Test accuracy:0.8369 mse:0.1136 roc:0.8696\n",
      "ROC improved from 0.0010 to 0.8696. Model savedd to result/tcenr_data/141/pre_trained32_32.ckpt\n",
      "epoch 12 took 0.32349443435668945 ms. avg loss 0.3458\n",
      "Train accuracy:0.8258 mse:0.1146 roc:0.8613\n",
      "Test accuracy:0.8409 mse:0.1077 roc:0.8778\n",
      "ROC improved from 0.8696 to 0.8778. Model savedd to result/tcenr_data/141/pre_trained32_32.ckpt\n",
      "epoch 13 took 0.42632460594177246 ms. avg loss 0.3424\n",
      "Train accuracy:0.8265 mse:0.1136 roc:0.8647\n",
      "Test accuracy:0.8366 mse:0.1123 roc:0.8687\n",
      "epoch 14 took 0.3113398551940918 ms. avg loss 0.3410\n",
      "Train accuracy:0.8263 mse:0.1136 roc:0.8655\n",
      "Test accuracy:0.8351 mse:0.1101 roc:0.8738\n",
      "epoch 15 took 0.37575316429138184 ms. avg loss 0.3405\n",
      "Train accuracy:0.8274 mse:0.1135 roc:0.8652\n",
      "Test accuracy:0.8379 mse:0.1085 roc:0.8787\n",
      "epoch 16 took 0.3388354778289795 ms. avg loss 0.3342\n",
      "Train accuracy:0.8306 mse:0.1116 roc:0.8702\n",
      "Test accuracy:0.8409 mse:0.1106 roc:0.8740\n",
      "epoch 17 took 0.28604936599731445 ms. avg loss 0.3318\n",
      "Train accuracy:0.8317 mse:0.1109 roc:0.8718\n",
      "Test accuracy:0.8462 mse:0.1067 roc:0.8812\n",
      "ROC improved from 0.8778 to 0.8812. Model savedd to result/tcenr_data/141/pre_trained32_32.ckpt\n",
      "epoch 18 took 0.4218451976776123 ms. avg loss 0.3363\n",
      "Train accuracy:0.8296 mse:0.1124 roc:0.8679\n",
      "Test accuracy:0.8453 mse:0.1061 roc:0.8816\n",
      "epoch 19 took 0.3124964237213135 ms. avg loss 0.3381\n",
      "Train accuracy:0.8251 mse:0.1133 roc:0.8643\n",
      "Test accuracy:0.8409 mse:0.1082 roc:0.8782\n",
      "epoch 20 took 0.265561580657959 ms. avg loss 0.3381\n",
      "Train accuracy:0.8272 mse:0.1125 roc:0.8673\n",
      "Test accuracy:0.8292 mse:0.1137 roc:0.8626\n",
      "epoch 21 took 0.31247806549072266 ms. avg loss 0.3423\n",
      "Train accuracy:0.8290 mse:0.1133 roc:0.8643\n",
      "Test accuracy:0.8248 mse:0.1195 roc:0.8447\n",
      "epoch 22 took 0.4530029296875 ms. avg loss 0.3372\n",
      "Train accuracy:0.8325 mse:0.1116 roc:0.8687\n",
      "Test accuracy:0.8325 mse:0.1089 roc:0.8796\n",
      "epoch 23 took 0.343670129776001 ms. avg loss 0.3297\n",
      "Train accuracy:0.8332 mse:0.1099 roc:0.8739\n",
      "Test accuracy:0.8475 mse:0.1050 roc:0.8881\n",
      "ROC improved from 0.8812 to 0.8881. Model savedd to result/tcenr_data/141/pre_trained32_32.ckpt\n",
      "epoch 24 took 0.2968742847442627 ms. avg loss 0.3251\n",
      "Train accuracy:0.8347 mse:0.1087 roc:0.8776\n",
      "Test accuracy:0.8450 mse:0.1071 roc:0.8822\n",
      "epoch 25 took 0.34374451637268066 ms. avg loss 0.3259\n",
      "Train accuracy:0.8333 mse:0.1093 roc:0.8761\n",
      "Test accuracy:0.8449 mse:0.1057 roc:0.8831\n",
      "epoch 26 took 0.2968113422393799 ms. avg loss 0.3276\n",
      "Train accuracy:0.8306 mse:0.1099 roc:0.8729\n",
      "Test accuracy:0.8502 mse:0.1045 roc:0.8867\n",
      "epoch 27 took 0.42177534103393555 ms. avg loss 0.3232\n",
      "Train accuracy:0.8353 mse:0.1081 roc:0.8782\n",
      "Test accuracy:0.8415 mse:0.1080 roc:0.8750\n",
      "epoch 28 took 0.34368062019348145 ms. avg loss 0.3259\n",
      "Train accuracy:0.8328 mse:0.1092 roc:0.8750\n",
      "Test accuracy:0.8421 mse:0.1063 roc:0.8779\n",
      "epoch 29 took 0.3593416213989258 ms. avg loss 0.3306\n",
      "Train accuracy:0.8243 mse:0.1116 roc:0.8663\n",
      "Test accuracy:0.8415 mse:0.1067 roc:0.8795\n",
      "epoch 30 took 0.343670129776001 ms. avg loss 0.3214\n",
      "Train accuracy:0.8314 mse:0.1082 roc:0.8770\n",
      "Test accuracy:0.8422 mse:0.1137 roc:0.8805\n",
      "epoch 31 took 0.34374117851257324 ms. avg loss 0.3236\n",
      "Train accuracy:0.8319 mse:0.1086 roc:0.8754\n",
      "Test accuracy:0.8422 mse:0.1085 roc:0.8798\n",
      "epoch 32 took 0.32811808586120605 ms. avg loss 0.3405\n",
      "Train accuracy:0.8230 mse:0.1134 roc:0.8598\n",
      "Test accuracy:0.8356 mse:0.1128 roc:0.8644\n",
      "epoch 33 took 0.3437221050262451 ms. avg loss 0.3434\n",
      "Train accuracy:0.8212 mse:0.1145 roc:0.8580\n",
      "Test accuracy:0.8404 mse:0.1103 roc:0.8729\n",
      "epoch 34 took 0.3593578338623047 ms. avg loss 0.3411\n",
      "Train accuracy:0.8239 mse:0.1134 roc:0.8611\n",
      "Test accuracy:0.8428 mse:0.1056 roc:0.8809\n",
      "epoch 35 took 0.32807397842407227 ms. avg loss 0.3270\n",
      "Train accuracy:0.8272 mse:0.1098 roc:0.8714\n",
      "Test accuracy:0.8426 mse:0.1061 roc:0.8783\n",
      "epoch 36 took 0.31249284744262695 ms. avg loss 0.3200\n",
      "Train accuracy:0.8324 mse:0.1074 roc:0.8797\n",
      "Test accuracy:0.8461 mse:0.1056 roc:0.8811\n",
      "epoch 37 took 0.2656259536743164 ms. avg loss 0.3186\n",
      "Train accuracy:0.8321 mse:0.1074 roc:0.8790\n",
      "Test accuracy:0.8451 mse:0.1072 roc:0.8818\n",
      "epoch 38 took 0.3437192440032959 ms. avg loss 0.3176\n",
      "Train accuracy:0.8331 mse:0.1067 roc:0.8814\n",
      "Test accuracy:0.8361 mse:0.1108 roc:0.8762\n",
      "epoch 39 took 0.3749501705169678 ms. avg loss 0.3249\n",
      "Train accuracy:0.8290 mse:0.1091 roc:0.8744\n",
      "Test accuracy:0.8365 mse:0.1120 roc:0.8660\n",
      "epoch 40 took 0.3436429500579834 ms. avg loss 0.3342\n",
      "Train accuracy:0.8233 mse:0.1127 roc:0.8609\n",
      "Test accuracy:0.8401 mse:0.1081 roc:0.8733\n",
      "epoch 41 took 0.3905775547027588 ms. avg loss 0.3213\n",
      "Train accuracy:0.8305 mse:0.1084 roc:0.8764\n",
      "Test accuracy:0.8453 mse:0.1054 roc:0.8886\n",
      "epoch 42 took 0.29680681228637695 ms. avg loss 0.3211\n",
      "Train accuracy:0.8281 mse:0.1082 roc:0.8744\n",
      "Test accuracy:0.8403 mse:0.1084 roc:0.8759\n",
      "epoch 43 took 0.2655620574951172 ms. avg loss 0.3206\n",
      "Train accuracy:0.8283 mse:0.1080 roc:0.8749\n",
      "Test accuracy:0.8390 mse:0.1076 roc:0.8769\n",
      "epoch 44 took 0.26563310623168945 ms. avg loss 0.3273\n",
      "Train accuracy:0.8252 mse:0.1103 roc:0.8686\n",
      "Test accuracy:0.8378 mse:0.1087 roc:0.8761\n",
      "Early stop due to no imporvement since epoch 23\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "best_roc_auc=0.001\n",
    "best_roc_epoch=MIN_EPOCH_TO_SAVE\n",
    "saver = tf.train.Saver({\"item_embedding\":emb_item_layer, \"user_embedding\":emb_user_layer})\n",
    "\n",
    "#TRAIN_SIZE = 256100\n",
    "#TEST_SIZE = 6764\n",
    "\n",
    "test_ratings, test_output,users,items = sess.run([in_ratings, predictions,in_user,in_item],get_feed_dict(test_set,0,TEST_SIZE))\n",
    "test_accuracy = accuracy_score(y_true = test_ratings, y_pred=test_output.round())\n",
    "test_mse = mean_squared_error(y_true=test_ratings, y_pred=test_output)\n",
    "test_roc_auc = roc_auc_score(y_true=test_ratings, y_score=test_output)\n",
    "print(\"Test accuracy before training :{:.4f} mse:{:.4f} roc:{:.4f}\".format(test_accuracy,test_mse,test_roc_auc))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    t1 = time()\n",
    "    # Shuffle the training batch order\n",
    "    training_indexes = list(range(0, TRAIN_SIZE, BATCH_SIZE))\n",
    "    total_batches = len(training_indexes)\n",
    "    #np.random.shuffle(training_indexes)\n",
    "    train_ratings=np.zeros(TRAIN_SIZE)\n",
    "    train_outputs=np.zeros(TRAIN_SIZE)\n",
    "    train_losses=np.zeros(total_batches)\n",
    "    # Train the model for each batch size\n",
    "    curr_iter=0\n",
    "    for start in training_indexes:\n",
    "        end = min(start + BATCH_SIZE,TRAIN_SIZE)\n",
    "        feed_dict = get_feed_dict(train_set,start,end)\n",
    "        # Perform a training step for current batch\n",
    "        _,curr_loss,curr_ratings, curr_output = sess.run([train_step,loss,in_ratings,predictions],feed_dict)\n",
    "        train_ratings[start:end] = curr_ratings\n",
    "        train_outputs[start:end] = curr_output\n",
    "        train_losses[curr_iter] = curr_loss\n",
    "        curr_iter+=1\n",
    "        \n",
    "    print(\"epoch {} took {} ms. avg loss {:.4f}\".format(epoch,time()-t1,np.average(train_losses)))\n",
    "    train_accuracy = accuracy_score(y_true = train_ratings, y_pred=train_outputs.round())\n",
    "    train_mse = mean_squared_error(y_true=train_ratings, y_pred=train_outputs)\n",
    "    train_roc_auc = roc_auc_score(y_true=train_ratings, y_score=train_outputs)\n",
    "    print(\"Train accuracy:{:.4f} mse:{:.4f} roc:{:.4f}\".format(train_accuracy,train_mse,train_roc_auc))\n",
    "\n",
    "    test_ratings, test_output,users,items = sess.run([in_ratings, predictions,in_user,in_item],get_feed_dict(test_set,0,TEST_SIZE))\n",
    "    test_accuracy = accuracy_score(y_true = test_ratings, y_pred=test_output.round())\n",
    "    test_mse = mean_squared_error(y_true=test_ratings, y_pred=test_output)\n",
    "    test_roc_auc = roc_auc_score(y_true=test_ratings, y_score=test_output)\n",
    "    print(\"Test accuracy:{:.4f} mse:{:.4f} roc:{:.4f}\".format(test_accuracy,test_mse,test_roc_auc))\n",
    "    \n",
    "    if (test_roc_auc/best_roc_auc)>ROC_DIFF_TO_SAVE and epoch>MIN_EPOCH_TO_SAVE:\n",
    "        save_path = saver.save(sess, WEIGHTS_DIR + \"pre_trained\" + str(ITEM_EMBEDDING_SIZE) + \"_\" + str(USER_EMBEDDING_SIZE) +\".ckpt\")\n",
    "        print(\"ROC improved from {:.4f} to {:.4f}. Model savedd to {}\".format(best_roc_auc,test_roc_auc,save_path))\n",
    "        best_roc_auc = test_roc_auc\n",
    "        best_roc_epoch = epoch\n",
    "    \n",
    "    if (epoch - best_roc_epoch)>EARLY_STOP_INTERVAL:\n",
    "        print(\"Early stop due to no imporvement since epoch {}\".format(best_roc_epoch))\n",
    "        break\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,mean_squared_error\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "def load_file(file):\n",
    "    users = []\n",
    "    pois = []\n",
    "    scores = []\n",
    "    historic_pois = []\n",
    "    reviews = []\n",
    "    for line in file:\n",
    "        record = line.split(\"\\t\")\n",
    "        user_id = int(record[0])\n",
    "        poi_id = int(record[1])\n",
    "        score = float(record[2])\n",
    "        last_k = [int(s) for s in record[4].split(',')]\n",
    "        review = record[5]\n",
    "        users.append(user_id)\n",
    "        pois.append(poi_id)\n",
    "        scores.append(score)\n",
    "        historic_pois.append(last_k)\n",
    "        reviews.append(review)\n",
    "    return users,pois,scores,historic_pois,reviews\n",
    "\n",
    "# Dataset stats\n",
    "PREFIX = \"414_t25_rest_b_\"#\"1655_t10_rest_\"\n",
    "DIR = \"amazon_dataset/\" + PREFIX #\"yelp_dataset/\" + PREFIX\n",
    "WEIGHTS_DIR = \"result/\" + PREFIX\n",
    "#NUM_USERS = 42475#3471#38039#7987#6869  \n",
    "#NUM_ITEMS = 42099#3996#50513#6153  \n",
    "#TRAIN_SIZE = 4771688#694836#4157256#1089484 \n",
    "#TEST_SIZE = 169900#13884#152156#27476 \n",
    "NUM_USERS = 14346 \n",
    "NUM_ITEMS = 34389\n",
    "TRAIN_SIZE = 885756\n",
    "TEST_SIZE = 57384 \n",
    "FIRST_K = 8\n",
    "NUM_FEATURES = 25#661#133\n",
    "FEATURES_PER_POI = 25#25\n",
    "FEATURE_TYPE = 'topics'\n",
    "\n",
    "# Load poi-categories dictionary\n",
    "with open(DIR + 'poi_' + FEATURE_TYPE + '.pkl', 'rb') as f:\n",
    "    poi_categories = pickle.load(f)\n",
    "\n",
    "# Flatten all categoires into one dimensional set\n",
    "categories_list = set()\n",
    "for categories in poi_categories.values():\n",
    "    for category in categories:\n",
    "        categories_list.add(category)\n",
    "    \n",
    "# Load training data\n",
    "with open(DIR + \"train.txt\") as train_file:\n",
    "    train_users, train_pois, train_scores, train_k_pois, train_reviews  = load_file(train_file)\n",
    "# Load categories of training pois\n",
    "train_features = []\n",
    "for poi in train_pois:\n",
    "    train_features.append(poi_categories[poi])\n",
    "# Set training dataset\n",
    "train_set = {'users': train_users, 'pois': train_pois, 'scores': train_scores, 'k_pois': train_k_pois, 'reviews': train_reviews, \n",
    "             'features': train_features}\n",
    "\n",
    "# Load test data\n",
    "with open(DIR + \"valid.txt\") as test_file:\n",
    "    test_users, test_pois, test_scores, test_k_pois, test_reviews = load_file(test_file)\n",
    "# Load categories of test pois\n",
    "test_features = []\n",
    "for poi in test_pois:\n",
    "    test_features.append(poi_categories[poi])\n",
    "# Set test dataset\n",
    "test_set = {'users': test_users, 'pois': test_pois, 'scores': test_scores, 'k_pois': test_k_pois, 'reviews': test_reviews, \n",
    "            'features': test_features}\n",
    "\n",
    "# Check training data\n",
    "assert len(train_users)==TRAIN_SIZE, \"mismatch in training set size {}!={}\".format(len(train_users),TRAIN_SIZE)\n",
    "assert len(train_k_pois[0])==FIRST_K==len(test_k_pois[0]), \"mismatch in number of positive items {}!={}!={}\".format(len(train_k_pois[0]),FIRST_K,len(test_k_pois[0]))\n",
    "assert max(train_users)==(NUM_USERS-1)==max(test_users), \"mimatch in number of users {}!={}!={}\".format(max(train_users),(NUM_USERS-1),max(test_users))\n",
    "assert max(train_pois)==(NUM_ITEMS-1)==max(test_pois), \"mismatch in number of item {}!={}!={}\".format(max(train_pois),(NUM_ITEMS-1),max(test_pois))\n",
    "#assert len(categories_list)+1==NUM_FEATURES, \"mismatch in number of features {}!={}\".format(len(categories_list)+1,NUM_FEATURES)\n",
    "#assert len(train_features[0])==FEATURES_PER_POI, \"mismatch in number of features per location {}!={}\".format(len(train_features[0]),FEATURES_PER_POI)\n",
    "assert len(test_users)==TEST_SIZE, \"mismatch in test set size {}!={}\".format(len(test_users),TEST_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
