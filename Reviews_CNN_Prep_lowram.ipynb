{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing record 10000\n",
      "Processing record 20000\n",
      "Processing record 30000\n",
      "Processing record 40000\n",
      "Processing record 50000\n",
      "Processing record 60000\n",
      "Processing record 70000\n",
      "Processing record 80000\n",
      "Processing record 90000\n",
      "Processing record 100000\n",
      "Processing record 110000\n",
      "Processing record 120000\n",
      "Processing record 130000\n",
      "Processing record 140000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def clean_review(review,word_set,max_size):\n",
    "    \"\"\"\n",
    "    Removes characters from the review string that are in the to_filter string\n",
    "    The output is a filtered, lower case representation of the review, splitted to words by space\n",
    "    \"\"\"\n",
    "    for c in TO_SPACE:\n",
    "        review = review.replace(c,' ')\n",
    "    review = review.split(' ')[0:max_size]\n",
    "\n",
    "    review_stripped = []\n",
    "    for word in review:\n",
    "        to_remove = False\n",
    "        for c in FILTERED_CHARS:\n",
    "            if c in word:\n",
    "                to_remove = True\n",
    "                break\n",
    "        if to_remove == False:\n",
    "            new_word = word.strip()\n",
    "            if len(new_word)>0:\n",
    "                review_stripped.append(new_word)\n",
    "    return review_stripped\n",
    "\n",
    "def get_glove_dict(file):\n",
    "    \"\"\"\n",
    "    Load glove dictionary file. \n",
    "    Glove is a pre-trained nlp embedding layer from https://nlp.stanford.edu/projects/glove/\n",
    "    \"\"\"\n",
    "    with open(file,encoding=\"utf8\") as glove:\n",
    "        return {l[0]: np.asarray(l[1:], dtype=\"float32\") for l in [line.split() for line in glove]}\n",
    "\n",
    "DIR = \"../data/\"\n",
    "WORK_DIR = \"data_cnn/\"\n",
    "OUTPUT_DIR = WORK_DIR + \"tcenr_data/\"#WORK_DIR + \"amazon_movies/\"\n",
    "FILE_NAME = DIR + \"yelp_filtered.json\"#DIR+\"reviews_Movies_and_TV_5.json\"#\"Electronics_5.json\"#\"yelp_review_restraunts.json\"\n",
    "GLOVE_FILE = DIR + \"glove.6B.\"\n",
    "REVIEW_MAX_SIZE = 500\n",
    "# Glove file sizeW\n",
    "WORD_EMBEDDING_FILE = 50\n",
    "# Characters to filter out of reviews\n",
    "FILTERED_CHARS = '[!\"#$%&()*+-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "TO_SPACE = '[.,\\t\\n]'\n",
    "\n",
    "glove_to_load = GLOVE_FILE + str(WORD_EMBEDDING_FILE) + \"d.txt\"\n",
    "glove_dictionary = get_glove_dict(glove_to_load)\n",
    "\n",
    "raw_words = {}\n",
    "for word in glove_dictionary.keys():\n",
    "    raw_words[word] = None\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "LIMIT = 0\n",
    "reviews= []\n",
    "scores = []\n",
    "users = []\n",
    "pois = []\n",
    "user_counts = {}\n",
    "poi_counts = {}\n",
    "word_dict = {}\n",
    "word_dict_rev = {}\n",
    "reviews_file = open(FILE_NAME,encoding=\"utf8\")\n",
    "data_loaded = 0\n",
    "word_count = 1\n",
    "\n",
    "for line in reviews_file:\n",
    "    data_loaded+=1\n",
    "    if (data_loaded == LIMIT):\n",
    "        break\n",
    "    if (data_loaded % 10000 == 0):\n",
    "        print(\"Processing record {}\".format(data_loaded))\n",
    "    record = json.loads(line)\n",
    "    score = record[\"stars\"]\n",
    "    review = record[\"text\"]\n",
    "    user = record[\"user_id\"]\n",
    "    poi = record[\"business_id\"]\n",
    "    #user = record[\"reviewerID\"]\n",
    "    #poi = record[\"asin\"]\n",
    "    #score = int(record[\"overall\"])\n",
    "    #review = record[\"reviewText\"]\n",
    "    for c in TO_SPACE:\n",
    "        review = review.replace(c,' ')\n",
    "    review = review.split(' ')[0:REVIEW_MAX_SIZE]\n",
    "    \n",
    "    new_review = []\n",
    "    \n",
    "    for word in review:\n",
    "        to_remove = False\n",
    "        word = word.strip().lower()\n",
    "        for c in FILTERED_CHARS:\n",
    "            if c in word:\n",
    "                to_remove = True\n",
    "                break\n",
    "        word = word.replace('\\'','')\n",
    "        if to_remove == False and word in raw_words:\n",
    "            \n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = word_count\n",
    "                word_dict_rev[word_count] = word\n",
    "                word_count+=1\n",
    "            new_review.append(word_dict[word])\n",
    "            \n",
    "    \n",
    "    if (len(new_review)>0):\n",
    "        reviews.append(new_review)\n",
    "        scores.append(score)\n",
    "        users.append(user)\n",
    "        if user not in user_counts:\n",
    "            user_counts[user] = 1\n",
    "        else:\n",
    "            user_counts[user] += 1\n",
    "        pois.append(poi)\n",
    "        if poi not in poi_counts:\n",
    "            poi_counts[poi] = 1\n",
    "        else:\n",
    "            poi_counts[poi] += 1\n",
    "        \n",
    "reviews_file.close()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "new_glove = {}\n",
    "new_glove[0] = np.array([0.0] * WORD_EMBEDDING_FILE)\n",
    "for word in word_dict:\n",
    "    word_idx = word_dict[word]\n",
    "    new_glove[word_idx] = glove_dictionary[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = []\n",
    "train_scores = []\n",
    "test_reviews = []\n",
    "test_scores = []\n",
    "for i in range(len(reviews)):\n",
    "    user = users[i]\n",
    "    poi = pois[i]\n",
    "    review = reviews[i]\n",
    "    if user_counts[user]>=10 and poi_counts[poi]>=10:\n",
    "        set_choice = random.random()\n",
    "        if (set_choice>0.1):\n",
    "            train_reviews.append(review)\n",
    "            train_scores.append(scores[i])\n",
    "        else:\n",
    "            test_reviews.append(review)\n",
    "            test_scores.append(scores[i])\n",
    "train_set = {\"reviews\": train_reviews, \"scores\": train_scores}\n",
    "test_set = {\"reviews\": test_reviews, \"scores\": test_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141019 139228\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews), len(train_reviews)+len(test_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:125376, Test size:13852\n",
      "dictionary size:58487\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:{}, Test size:{}\".format(len(train_reviews),len(test_scores)))\n",
    "print(\"dictionary size:{}\".format(len(word_dict)))\n",
    "\n",
    "with open(OUTPUT_DIR + \"glove_file.pkl\",'wb') as file:\n",
    "    pickle.dump(list(new_glove.values()),file)\n",
    "    \n",
    "with open(OUTPUT_DIR + \"word_dictionary.pkl\",'wb') as file:\n",
    "    pickle.dump(word_dict,file)\n",
    "    \n",
    "with open(OUTPUT_DIR + \"word_dictionary_reverse.pkl\",'wb') as file:\n",
    "    pickle.dump(word_dict_rev,file)\n",
    "    \n",
    "with open(OUTPUT_DIR + \"train.pkl\",'wb') as file:\n",
    "    pickle.dump(train_set,file)\n",
    "    \n",
    "with open(OUTPUT_DIR + \"test.pkl\",'wb') as file:\n",
    "    pickle.dump(test_set,file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58488"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = WORK_DIR + \"amazon/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
